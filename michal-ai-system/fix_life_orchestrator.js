#!/usr/bin/env node

/**
 * Life Orchestrator - Complete System Fix
 * ========================================
 * ◊°◊ß◊®◊ô◊§◊ò ◊ñ◊î ◊û◊™◊ß◊ü ◊ê◊™ ◊õ◊ú ◊î◊ë◊¢◊ô◊ï◊™ ◊©◊†◊û◊¶◊ê◊ï ◊ë◊û◊¢◊®◊õ◊™
 * 
 * ◊ë◊¢◊ô◊ï◊™ ◊©◊†◊û◊¶◊ê◊ï:
 * 1. AgentCore.js ◊ó◊°◊®
 * 2. ◊û◊¢◊®◊õ◊™ State ◊ú◊ê ◊û◊ô◊ï◊©◊û◊™
 * 3. ◊ê◊ô◊ü Knowledge Graph
 * 4. ◊ê◊ô◊ü ◊°◊ï◊õ◊†◊ô◊ù ◊û◊™◊û◊ó◊ô◊ù
 * 5. Python agent ◊ú◊ê ◊û◊°◊ï◊†◊õ◊®◊ü
 * 
 * ◊î◊®◊¶◊î: node fix_life_orchestrator.js
 */

const fs = require('fs');
const path = require('path');
const { execSync } = require('child_process');

console.log(`
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë        Life Orchestrator - Complete System Fix          ‚ïë
‚ïë                    Version 1.0.0                         ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
`);

// ============= Step 1: Create Missing AgentCore =============

console.log('üì¶ Step 1: Creating AgentCore.js with full functionality...\n');

const agentCoreContent = `/**
 * AgentCore - The Brain of Life Orchestrator
 * ===========================================
 */

class MemorySystem {
    constructor() {
        this.shortTerm = new Map();
        this.longTerm = new Map();
        this.episodic = [];
        this.patterns = new Map();
    }

    remember(key, value, type = 'short') {
        const memory = {
            key,
            value,
            timestamp: new Date(),
            accessCount: 0,
            importance: this.calculateImportance(value)
        };

        if (type === 'short') {
            this.shortTerm.set(key, memory);
            // Move to long-term if accessed frequently
            if (memory.accessCount > 5) {
                this.longTerm.set(key, memory);
            }
        } else {
            this.longTerm.set(key, memory);
        }
    }

    recall(key) {
        let memory = this.shortTerm.get(key) || this.longTerm.get(key);
        if (memory) {
            memory.accessCount++;
            memory.lastAccessed = new Date();
            return memory.value;
        }
        return null;
    }

    calculateImportance(value) {
        let score = 5;
        if (value.deadline) {
            const daysLeft = Math.ceil((new Date(value.deadline) - new Date()) / 86400000);
            if (daysLeft <= 1) score = 10;
            else if (daysLeft <= 3) score = 8;
            else if (daysLeft <= 7) score = 6;
        }
        if (value.amount && value.amount > 1000) score += 2;
        if (value.priority === 'critical') score = 10;
        return Math.min(score, 10);
    }

    findPattern(events) {
        // Simple pattern detection
        const timePatterns = {};
        events.forEach(event => {
            const hour = new Date(event.timestamp).getHours();
            timePatterns[hour] = (timePatterns[hour] || 0) + 1;
        });
        
        const mostActiveHour = Object.entries(timePatterns)
            .sort((a, b) => b[1] - a[1])[0];
        
        return {
            mostActiveHour: mostActiveHour ? parseInt(mostActiveHour[0]) : null,
            totalEvents: events.length
        };
    }
}

class KnowledgeGraph {
    constructor() {
        this.nodes = new Map();
        this.edges = [];
        this.index = new Map();
    }

    addNode(type, data) {
        const id = \`\${type}_\${data.id || Date.now()}\`;
        const node = {
            id,
            type,
            data,
            created: new Date(),
            connections: []
        };
        
        this.nodes.set(id, node);
        
        // Index by type
        if (!this.index.has(type)) {
            this.index.set(type, []);
        }
        this.index.get(type).push(id);
        
        // Auto-connect related nodes
        this.autoConnect(node);
        
        return id;
    }

    autoConnect(newNode) {
        // Connect to related existing nodes
        this.nodes.forEach((node, id) => {
            if (node.id !== newNode.id) {
                const relationship = this.detectRelationship(newNode, node);
                if (relationship) {
                    this.addEdge(newNode.id, node.id, relationship);
                }
            }
        });
    }

    detectRelationship(node1, node2) {
        const data1 = node1.data;
        const data2 = node2.data;
        
        // Same client/person
        if (data1.client && data2.client && data1.client === data2.client) {
            return 'SAME_CLIENT';
        }
        
        // Same authority/company
        if (data1.authority && data2.authority && data1.authority === data2.authority) {
            return 'SAME_AUTHORITY';
        }
        
        // Temporal relationship
        if (data1.deadline && data2.deadline) {
            const diff = Math.abs(new Date(data1.deadline) - new Date(data2.deadline));
            if (diff < 86400000) return 'SAME_DAY';
            if (diff < 604800000) return 'SAME_WEEK';
        }
        
        // Financial relationship
        if (data1.creditor && data2.creditor && data1.creditor === data2.creditor) {
            return 'SAME_CREDITOR';
        }
        
        return null;
    }

    addEdge(from, to, type, metadata = {}) {
        const edge = {
            from,
            to,
            type,
            metadata,
            created: new Date()
        };
        
        this.edges.push(edge);
        
        // Update node connections
        const fromNode = this.nodes.get(from);
        const toNode = this.nodes.get(to);
        
        if (fromNode) fromNode.connections.push({ to, type });
        if (toNode) toNode.connections.push({ from, type });
    }

    findPath(from, to) {
        // Simple BFS pathfinding
        const visited = new Set();
        const queue = [[from]];
        
        while (queue.length > 0) {
            const path = queue.shift();
            const current = path[path.length - 1];
            
            if (current === to) return path;
            
            if (!visited.has(current)) {
                visited.add(current);
                const node = this.nodes.get(current);
                
                if (node) {
                    node.connections.forEach(conn => {
                        queue.push([...path, conn.to]);
                    });
                }
            }
        }
        
        return null;
    }

    getContext(nodeId, depth = 2) {
        const context = {
            node: this.nodes.get(nodeId),
            related: [],
            patterns: []
        };
        
        if (!context.node) return context;
        
        // Get connected nodes up to depth
        const visited = new Set();
        const queue = [{ id: nodeId, level: 0 }];
        
        while (queue.length > 0) {
            const { id, level } = queue.shift();
            
            if (level > depth || visited.has(id)) continue;
            visited.add(id);
            
            const node = this.nodes.get(id);
            if (node && id !== nodeId) {
                context.related.push({ node, level });
            }
            
            if (node && level < depth) {
                node.connections.forEach(conn => {
                    queue.push({ id: conn.to || conn.from, level: level + 1 });
                });
            }
        }
        
        return context;
    }
}

class AgentCore {
    constructor() {
        this.memory = new MemorySystem();
        this.graph = new KnowledgeGraph();
        this.state = {
            currentFocus: null,
            activeProcesses: [],
            pendingDecisions: [],
            userProfile: {}
        };
    }

    static instance = null;

    static getInstance() {
        if (!AgentCore.instance) {
            AgentCore.instance = new AgentCore();
        }
        return AgentCore.instance;
    }

    static ingestInitial(appData) {
        const instance = AgentCore.getInstance();
        instance.ingest(appData);
    }

    ingest(appData) {
        // Ingest all data into knowledge graph and memory
        ['tasks', 'debts', 'bureaucracy', 'emails'].forEach(category => {
            if (appData[category]) {
                appData[category].forEach(item => {
                    const nodeId = this.graph.addNode(category, item);
                    this.memory.remember(\`\${category}_\${item.id}\`, item);
                    
                    // Track active processes
                    if (item.status && item.status !== 'completed') {
                        this.state.activeProcesses.push({
                            id: nodeId,
                            type: category,
                            item
                        });
                    }
                });
            }
        });
        
        console.log(\`üìä Ingested \${this.graph.nodes.size} items into knowledge graph\`);
    }

    static getPriorities(appData) {
        const instance = AgentCore.getInstance();
        return instance.calculatePriorities(appData);
    }

    calculatePriorities(appData) {
        const priorities = [];
        const now = new Date();
        
        // Process all items and calculate priority scores
        ['tasks', 'debts', 'bureaucracy'].forEach(category => {
            if (appData[category]) {
                appData[category].forEach(item => {
                    let score = 0;
                    let factors = [];
                    
                    // Deadline factor
                    if (item.deadline) {
                        const daysLeft = Math.ceil((new Date(item.deadline) - now) / 86400000);
                        if (daysLeft <= 0) {
                            score += 100;
                            factors.push('overdue');
                        } else if (daysLeft <= 1) {
                            score += 90;
                            factors.push('due_today');
                        } else if (daysLeft <= 3) {
                            score += 70;
                            factors.push('due_soon');
                        } else if (daysLeft <= 7) {
                            score += 50;
                            factors.push('this_week');
                        } else {
                            score += 20;
                        }
                    }
                    
                    // Priority factor
                    if (item.priority === 'critical' || item.priority === '◊ì◊ó◊ï◊£') score += 50;
                    else if (item.priority === 'high' || item.priority === '◊í◊ë◊ï◊î') score += 30;
                    else if (item.priority === 'medium' || item.priority === '◊ë◊ô◊†◊ï◊†◊ô') score += 10;
                    
                    // Financial factor
                    if (item.amount || item.value) {
                        const amount = item.amount || item.value;
                        if (amount > 5000) score += 30;
                        else if (amount > 1000) score += 20;
                        else if (amount > 500) score += 10;
                    }
                    
                    // Status factor
                    if (item.status === 'blocked') score += 40;
                    else if (item.status === 'waiting') score += 20;
                    
                    // Context from knowledge graph
                    const context = this.graph.getContext(\`\${category}_\${item.id}\`);
                    if (context.related.length > 3) {
                        score += 15; // Many dependencies
                        factors.push('complex');
                    }
                    
                    priorities.push({
                        ...item,
                        category,
                        priorityScore: score,
                        factors,
                        context
                    });
                });
            }
        });
        
        // Sort by priority score
        priorities.sort((a, b) => b.priorityScore - a.priorityScore);
        
        return priorities;
    }

    static generateQuestions(appData) {
        const instance = AgentCore.getInstance();
        return instance.generateContextualQuestions(appData);
    }

    generateContextualQuestions(appData) {
        const questions = [];
        const patterns = this.memory.findPattern(this.state.activeProcesses);
        
        // Questions based on incomplete information
        this.state.activeProcesses.forEach(process => {
            if (!process.item.estimated_completion) {
                questions.push({
                    id: \`q_\${Date.now()}_1\`,
                    type: 'estimation',
                    text: \`◊û◊™◊ô ◊ê◊™ ◊û◊¢◊®◊ô◊õ◊î ◊©◊™◊°◊ô◊ô◊û◊ô ◊ê◊™ \${process.item.project || process.item.task}?\`,
                    context: process
                });
            }
            
            if (process.item.status === 'blocked' && !process.item.blocker_reason) {
                questions.push({
                    id: \`q_\${Date.now()}_2\`,
                    type: 'blocker',
                    text: \`◊û◊î ◊ó◊ï◊°◊ù ◊ê◊™ \${process.item.project || process.item.task}?\`,
                    context: process
                });
            }
        });
        
        // Questions based on patterns
        if (patterns.mostActiveHour) {
            questions.push({
                id: \`q_\${Date.now()}_3\`,
                type: 'pattern',
                text: \`◊©◊û◊™◊ô ◊ú◊ë ◊©◊ê◊™ ◊î◊õ◊ô ◊§◊¢◊ô◊ú◊î ◊ë◊©◊¢◊î \${patterns.mostActiveHour}:00. ◊î◊ê◊ù ◊ú◊î◊™◊û◊ß◊ì ◊ê◊ñ ◊ë◊û◊©◊ô◊û◊ï◊™ ◊î◊ó◊©◊ï◊ë◊ï◊™?\`,
                context: patterns
            });
        }
        
        // Strategic questions
        const criticalItems = this.calculatePriorities(appData).filter(p => p.priorityScore > 80);
        if (criticalItems.length > 3) {
            questions.push({
                id: \`q_\${Date.now()}_4\`,
                type: 'strategy',
                text: '◊ô◊© ◊ú◊ö \${criticalItems.length} ◊û◊©◊ô◊û◊ï◊™ ◊ß◊®◊ô◊ò◊ô◊ï◊™. ◊ê◊ô◊ñ◊ï ◊î◊õ◊ô ◊ó◊©◊ï◊ë◊î ◊ú◊ö ◊õ◊®◊í◊¢?',
                options: criticalItems.map(item => item.project || item.task || item.creditor)
            });
        }
        
        return questions;
    }

    static runSyncSimulation(sources = ['emails', 'calendar', 'documents']) {
        const instance = AgentCore.getInstance();
        return instance.simulateSync(sources);
    }

    simulateSync(sources) {
        const results = [];
        
        sources.forEach(source => {
            const items = Math.floor(Math.random() * 5) + 1;
            const duration = 300 + Math.floor(Math.random() * 700);
            
            results.push({
                source,
                items,
                duration,
                status: 'success',
                newData: this.generateMockSyncData(source, items)
            });
            
            // Simulate ingesting new data
            if (source === 'emails') {
                for (let i = 0; i < items; i++) {
                    this.graph.addNode('email', {
                        id: \`email_\${Date.now()}_\${i}\`,
                        subject: \`New email \${i}\`,
                        from: 'system@example.com',
                        importance: Math.random() > 0.5 ? 'high' : 'normal'
                    });
                }
            }
        });
        
        this.memory.remember('last_sync', {
            timestamp: new Date(),
            results
        });
        
        return { results, summary: \`Synced \${results.reduce((a, r) => a + r.items, 0)} items\` };
    }

    generateMockSyncData(source, count) {
        const data = [];
        for (let i = 0; i < count; i++) {
            switch(source) {
                case 'emails':
                    data.push({
                        subject: \`Email \${i}\`,
                        from: \`sender\${i}@example.com\`,
                        date: new Date()
                    });
                    break;
                case 'calendar':
                    data.push({
                        title: \`Event \${i}\`,
                        date: new Date(Date.now() + i * 86400000)
                    });
                    break;
                default:
                    data.push({ type: source, index: i });
            }
        }
        return data;
    }

    static stateSnapshot(appData) {
        const instance = AgentCore.getInstance();
        return instance.getSnapshot(appData);
    }

    getSnapshot(appData) {
        return {
            memory: {
                shortTerm: Array.from(this.memory.shortTerm.entries()).slice(-10),
                patterns: this.memory.findPattern(this.state.activeProcesses),
                stats: {
                    shortTermSize: this.memory.shortTerm.size,
                    longTermSize: this.memory.longTerm.size,
                    episodicCount: this.memory.episodic.length
                }
            },
            knowledgeGraph: {
                nodes: this.graph.nodes.size,
                edges: this.graph.edges.length,
                nodeTypes: Array.from(this.graph.index.keys())
            },
            state: this.state,
            priorities: this.calculatePriorities(appData).slice(0, 5)
        };
    }

    static metrics(appData) {
        const instance = AgentCore.getInstance();
        return {
            timestamp: new Date().toISOString(),
            memory: {
                shortTerm: instance.memory.shortTerm.size,
                longTerm: instance.memory.longTerm.size
            },
            graph: {
                nodes: instance.graph.nodes.size,
                edges: instance.graph.edges.length
            },
            activeProcesses: instance.state.activeProcesses.length,
            pendingDecisions: instance.state.pendingDecisions.length
        };
    }

    static generateAutoActions(appData) {
        const instance = AgentCore.getInstance();
        return instance.suggestActions(appData);
    }

    suggestActions(appData) {
        const actions = [];
        const priorities = this.calculatePriorities(appData);
        
        priorities.slice(0, 5).forEach(item => {
            if (item.factors.includes('overdue')) {
                actions.push({
                    type: 'urgent',
                    action: 'send_reminder',
                    target: item,
                    message: \`‚ö†Ô∏è \${item.project || item.task} ◊ë◊ê◊ô◊ó◊ï◊®! ◊¶◊®◊ô◊ö ◊ú◊ò◊§◊ú ◊ì◊ó◊ï◊£\`
                });
            }
            
            if (item.category === 'debt' && item.factors.includes('due_soon')) {
                actions.push({
                    type: 'preparation',
                    action: 'prepare_objection',
                    target: item,
                    message: \`◊ú◊î◊õ◊ô◊ü ◊î◊™◊†◊í◊ì◊ï◊™ ◊ú-\${item.creditor}\`
                });
            }
            
            if (item.status === 'blocked') {
                actions.push({
                    type: 'unblock',
                    action: 'investigate_blocker',
                    target: item,
                    message: \`◊ú◊ë◊ì◊ï◊ß ◊û◊î ◊ó◊ï◊°◊ù ◊ê◊™ \${item.project || item.task}\`
                });
            }
        });
        
        return actions;
    }

    static updateFinancialBalance(balance) {
        const instance = AgentCore.getInstance();
        instance.memory.remember('financial_balance', {
            amount: balance,
            updated: new Date()
        }, 'long');
        instance.state.userProfile.currentBalance = balance;
        return { success: true, stored: balance };
    }
}

// Memory persistence
AgentCore.prototype.memory.persist = function() {
    try {
        const dataDir = path.join(__dirname, 'data');
        if (!fs.existsSync(dataDir)) {
            fs.mkdirSync(dataDir, { recursive: true });
        }
        
        const memoryData = {
            shortTerm: Array.from(this.shortTerm.entries()),
            longTerm: Array.from(this.longTerm.entries()),
            episodic: this.episodic,
            patterns: Array.from(this.patterns.entries())
        };
        
        fs.writeFileSync(
            path.join(dataDir, 'memory.json'),
            JSON.stringify(memoryData, null, 2)
        );
        
        return true;
    } catch (error) {
        console.error('Failed to persist memory:', error);
        return false;
    }
};

// Load persisted memory on init
AgentCore.prototype.memory.load = function() {
    try {
        const memoryFile = path.join(__dirname, 'data', 'memory.json');
        if (fs.existsSync(memoryFile)) {
            const data = JSON.parse(fs.readFileSync(memoryFile, 'utf8'));
            
            if (data.shortTerm) {
                this.shortTerm = new Map(data.shortTerm);
            }
            if (data.longTerm) {
                this.longTerm = new Map(data.longTerm);
            }
            if (data.episodic) {
                this.episodic = data.episodic;
            }
            if (data.patterns) {
                this.patterns = new Map(data.patterns);
            }
            
            console.log('‚úÖ Loaded persisted memory');
            return true;
        }
    } catch (error) {
        console.error('Failed to load memory:', error);
    }
    return false;
};

// Auto-load on creation
AgentCore.getInstance().memory.load();

module.exports = AgentCore;
`;

// Create services directory if not exists
if (!fs.existsSync('services')) {
    fs.mkdirSync('services');
}

// Write AgentCore
fs.writeFileSync('services/AgentCore.js', agentCoreContent);
console.log('‚úÖ Created comprehensive AgentCore.js with Memory, Knowledge Graph, and Prioritization\n');

// ============= Step 2: Fix Python Agent =============

console.log('üì¶ Step 2: Updating Python Agent for Life Orchestrator...\n');

const pythonAgentContent = `"""
Life Orchestrator - Intelligent Life Management Agent
=====================================================
Complete rewrite to match the Life Orchestrator architecture
"""

import os
import json
import asyncio
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional
from enum import Enum
from dataclasses import dataclass, field
import logging

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ==================== Data Models ====================

class EntityType(Enum):
    PERSON = "person"
    TASK = "task"
    DOCUMENT = "document"
    EVENT = "event"
    GOAL = "goal"
    CONSTRAINT = "constraint"
    RESOURCE = "resource"
    PROCESS = "process"
    DEADLINE = "deadline"

class RelationType(Enum):
    REQUIRES = "requires"
    BLOCKS = "blocks"
    ENABLES = "enables"
    CONFLICTS = "conflicts"
    RELATED = "related"
    INFLUENCES = "influences"
    OWNED_BY = "owned_by"
    RESPONSIBLE_FOR = "responsible"
    DEPENDS_ON = "depends_on"

@dataclass
class ContextNode:
    id: str
    type: EntityType
    data: Dict[str, Any]
    status: str = "active"
    confidence: float = 1.0
    created_at: datetime = field(default_factory=datetime.now)
    updated_at: datetime = field(default_factory=datetime.now)
    
    def to_dict(self):
        return {
            "id": self.id,
            "type": self.type.value,
            "data": self.data,
            "status": self.status,
            "confidence": self.confidence,
            "created_at": self.created_at.isoformat(),
            "updated_at": self.updated_at.isoformat()
        }

@dataclass
class ContextEdge:
    from_node: str
    to_node: str
    type: RelationType
    strength: float = 1.0
    metadata: Dict[str, Any] = field(default_factory=dict)
    created_at: datetime = field(default_factory=datetime.now)

# ==================== Context Graph ====================

class ContextGraph:
    def __init__(self):
        self.nodes: Dict[str, ContextNode] = {}
        self.edges: List[ContextEdge] = []
        self.index: Dict[EntityType, List[str]] = {}
    
    def add_node(self, node: ContextNode):
        self.nodes[node.id] = node
        if node.type not in self.index:
            self.index[node.type] = []
        self.index[node.type].append(node.id)
        logger.info(f"Added node: {node.id} of type {node.type}")
    
    def add_edge(self, edge: ContextEdge):
        self.edges.append(edge)
        logger.info(f"Added edge: {edge.from_node} -> {edge.to_node} ({edge.type})")
    
    def find_related(self, node_id: str, depth: int = 2) -> List[ContextNode]:
        """Find all nodes related to a given node up to specified depth"""
        related = []
        visited = set()
        queue = [(node_id, 0)]
        
        while queue:
            current_id, current_depth = queue.pop(0)
            if current_id in visited or current_depth > depth:
                continue
            
            visited.add(current_id)
            if current_id != node_id and current_id in self.nodes:
                related.append(self.nodes[current_id])
            
            # Find connected nodes
            for edge in self.edges:
                if edge.from_node == current_id:
                    queue.append((edge.to_node, current_depth + 1))
                elif edge.to_node == current_id:
                    queue.append((edge.from_node, current_depth + 1))
        
        return related

# ==================== Memory System ====================

class MemoryBank:
    def __init__(self):
        self.short_term = {}
        self.long_term = {}
        self.episodic = []
        self.patterns = {}
    
    def store(self, key: str, value: Any, memory_type: str = "short"):
        memory_item = {
            "value": value,
            "timestamp": datetime.now(),
            "access_count": 0,
            "importance": self._calculate_importance(value)
        }
        
        if memory_type == "short":
            self.short_term[key] = memory_item
        else:
            self.long_term[key] = memory_item
    
    def recall(self, key: str) -> Optional[Any]:
        if key in self.short_term:
            self.short_term[key]["access_count"] += 1
            return self.short_term[key]["value"]
        elif key in self.long_term:
            self.long_term[key]["access_count"] += 1
            return self.long_term[key]["value"]
        return None
    
    def _calculate_importance(self, value: Any) -> float:
        """Calculate importance score for memory item"""
        score = 5.0
        
        if isinstance(value, dict):
            if "deadline" in value:
                days_left = (datetime.fromisoformat(value["deadline"]) - datetime.now()).days
                if days_left <= 1:
                    score = 10.0
                elif days_left <= 3:
                    score = 8.0
                elif days_left <= 7:
                    score = 6.0
            
            if "priority" in value and value["priority"] == "critical":
                score = 10.0
            
            if "amount" in value and value["amount"] > 1000:
                score += 2.0
        
        return min(score, 10.0)

# ==================== Decision Engine ====================

class DecisionEngine:
    def __init__(self, graph: ContextGraph, memory: MemoryBank):
        self.graph = graph
        self.memory = memory
    
    def analyze_situation(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """Analyze current situation and suggest actions"""
        decisions = []
        urgent_items = []
        opportunities = []
        conflicts = []
        
        # Check for urgent deadlines
        for node_id, node in self.graph.nodes.items():
            if "deadline" in node.data:
                deadline = datetime.fromisoformat(node.data["deadline"])
                days_left = (deadline - datetime.now()).days
                
                if days_left <= 0:
                    urgent_items.append({
                        "node": node,
                        "urgency": "overdue",
                        "action": "immediate_action_required"
                    })
                elif days_left <= 3:
                    urgent_items.append({
                        "node": node,
                        "urgency": "critical",
                        "action": "prioritize_today"
                    })
        
        # Check for blocked processes
        for edge in self.graph.edges:
            if edge.type == RelationType.BLOCKS:
                blocker = self.graph.nodes.get(edge.from_node)
                blocked = self.graph.nodes.get(edge.to_node)
                
                if blocker and blocked:
                    conflicts.append({
                        "blocker": blocker,
                        "blocked": blocked,
                        "suggestion": f"Resolve {blocker.id} to unblock {blocked.id}"
                    })
        
        # Find optimization opportunities
        for node_type, node_ids in self.graph.index.items():
            if len(node_ids) > 3 and node_type == EntityType.TASK:
                similar_tasks = [self.graph.nodes[nid] for nid in node_ids[:3]]
                opportunities.append({
                    "type": "batch_processing",
                    "tasks": similar_tasks,
                    "suggestion": "Consider handling these similar tasks together"
                })
        
        return {
            "urgent": urgent_items,
            "opportunities": opportunities,
            "conflicts": conflicts,
            "recommended_actions": self._generate_recommendations(urgent_items, opportunities, conflicts)
        }
    
    def _generate_recommendations(self, urgent, opportunities, conflicts):
        recommendations = []
        
        # Handle urgent items first
        for item in urgent[:3]:  # Top 3 urgent
            recommendations.append({
                "priority": 1,
                "action": f"Handle {item['node'].data.get('title', item['node'].id)}",
                "reason": f"Status: {item['urgency']}",
                "estimated_time": "30-60 minutes"
            })
        
        # Then conflicts
        for conflict in conflicts[:2]:  # Top 2 conflicts
            recommendations.append({
                "priority": 2,
                "action": conflict["suggestion"],
                "reason": "Unblocking dependent tasks",
                "estimated_time": "15-30 minutes"
            })
        
        # Then opportunities
        for opp in opportunities[:1]:  # Top opportunity
            recommendations.append({
                "priority": 3,
                "action": opp["suggestion"],
                "reason": "Efficiency optimization",
                "estimated_time": "Variable"
            })
        
        return recommendations

# ==================== Main Life Orchestrator ====================

class LifeOrchestrator:
    def __init__(self):
        self.graph = ContextGraph()
        self.memory = MemoryBank()
        self.decision_engine = DecisionEngine(self.graph, self.memory)
        self.is_running = False
        
        logger.info("Life Orchestrator initialized")
    
    async def perceive(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Process new information and update internal state"""
        perception_result = {
            "received": input_data,
            "processed_nodes": [],
            "new_edges": [],
            "insights": []
        }
        
        # Process different types of input
        if "email" in input_data:
            node = self._process_email(input_data["email"])
            perception_result["processed_nodes"].append(node)
        
        if "task" in input_data:
            node = self._process_task(input_data["task"])
            perception_result["processed_nodes"].append(node)
        
        if "deadline" in input_data:
            node = self._process_deadline(input_data["deadline"])
            perception_result["processed_nodes"].append(node)
        
        # Auto-detect relationships
        self._detect_relationships()
        
        return perception_result
    
    def _process_email(self, email_data: Dict) -> ContextNode:
        """Process email and extract relevant information"""
        node = ContextNode(
            id=f"email_{datetime.now().timestamp()}",
            type=EntityType.DOCUMENT,
            data={
                "subject": email_data.get("subject", ""),
                "from": email_data.get("from", ""),
                "content": email_data.get("content", ""),
                "received": datetime.now().isoformat()
            }
        )
        
        self.graph.add_node(node)
        
        # Extract entities from email
        if "deadline" in email_data.get("content", "").lower():
            # Create deadline node
            deadline_node = ContextNode(
                id=f"deadline_{datetime.now().timestamp()}",
                type=EntityType.DEADLINE,
                data={"source": node.id, "extracted": True}
            )
            self.graph.add_node(deadline_node)
            self.graph.add_edge(ContextEdge(
                from_node=node.id,
                to_node=deadline_node.id,
                type=RelationType.RELATED
            ))
        
        return node
    
    def _process_task(self, task_data: Dict) -> ContextNode:
        """Process task information"""
        node = ContextNode(
            id=f"task_{task_data.get('id', datetime.now().timestamp())}",
            type=EntityType.TASK,
            data=task_data
        )
        
        self.graph.add_node(node)
        
        # Check for dependencies
        if "depends_on" in task_data:
            for dep_id in task_data["depends_on"]:
                self.graph.add_edge(ContextEdge(
                    from_node=dep_id,
                    to_node=node.id,
                    type=RelationType.BLOCKS
                ))
        
        return node
    
    def _process_deadline(self, deadline_data: Dict) -> ContextNode:
        """Process deadline information"""
        node = ContextNode(
            id=f"deadline_{deadline_data.get('id', datetime.now().timestamp())}",
            type=EntityType.DEADLINE,
            data=deadline_data
        )
        
        self.graph.add_node(node)
        self.memory.store(f"deadline_{node.id}", deadline_data, "long")
        
        return node
    
    def _detect_relationships(self):
        """Automatically detect relationships between nodes"""
        nodes_list = list(self.graph.nodes.values())
        
        for i, node1 in enumerate(nodes_list):
            for node2 in nodes_list[i+1:]:
                # Check for same person/entity
                if node1.data.get("client") == node2.data.get("client"):
                    self.graph.add_edge(ContextEdge(
                        from_node=node1.id,
                        to_node=node2.id,
                        type=RelationType.RELATED,
                        metadata={"reason": "same_client"}
                    ))
                
                # Check for temporal proximity
                if "deadline" in node1.data and "deadline" in node2.data:
                    d1 = datetime.fromisoformat(node1.data["deadline"])
                    d2 = datetime.fromisoformat(node2.data["deadline"])
                    
                    if abs((d1 - d2).days) <= 1:
                        self.graph.add_edge(ContextEdge(
                            from_node=node1.id,
                            to_node=node2.id,
                            type=RelationType.RELATED,
                            metadata={"reason": "same_timeframe"}
                        ))
    
    async def decide(self) -> Dict[str, Any]:
        """Make decisions based on current state"""
        analysis = self.decision_engine.analyze_situation({
            "graph_state": len(self.graph.nodes),
            "memory_state": len(self.memory.short_term)
        })
        
        return {
            "analysis": analysis,
            "timestamp": datetime.now().isoformat(),
            "confidence": 0.85
        }
    
    async def act(self, decision: Dict[str, Any]) -> Dict[str, Any]:
        """Execute actions based on decisions"""
        actions_taken = []
        
        for recommendation in decision.get("analysis", {}).get("recommended_actions", []):
            action_result = {
                "action": recommendation["action"],
                "status": "simulated",  # In real system, would execute
                "timestamp": datetime.now().isoformat()
            }
            actions_taken.append(action_result)
            
            # Store action in episodic memory
            self.memory.episodic.append(action_result)
        
        return {"actions_taken": actions_taken}
    
    async def learn(self, feedback: Dict[str, Any]):
        """Learn from feedback and update patterns"""
        # Extract patterns from feedback
        if "success" in feedback:
            pattern_key = f"pattern_{datetime.now().date()}"
            
            if pattern_key not in self.memory.patterns:
                self.memory.patterns[pattern_key] = []
            
            self.memory.patterns[pattern_key].append({
                "action": feedback.get("action"),
                "success": feedback["success"],
                "context": feedback.get("context", {})
            })
    
    async def process_message(self, message: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Main entry point for processing user messages"""
        logger.info(f"Processing message: {message}")
        
        # Perceive
        perception = await self.perceive({
            "message": message,
            "context": context,
            "timestamp": datetime.now().isoformat()
        })
        
        # Decide
        decision = await self.decide()
        
        # Act
        actions = await self.act(decision)
        
        # Generate response
        response = self._generate_response(perception, decision, actions)
        
        return {
            "response": response,
            "perception": perception,
            "decision": decision,
            "actions": actions,
            "graph_state": {
                "nodes": len(self.graph.nodes),
                "edges": len(self.graph.edges)
            },
            "memory_state": {
                "short_term": len(self.memory.short_term),
                "long_term": len(self.memory.long_term),
                "patterns": len(self.memory.patterns)
            }
        }
    
    def _generate_response(self, perception, decision, actions):
        """Generate human-friendly response"""
        urgent = decision.get("analysis", {}).get("urgent", [])
        recommendations = decision.get("analysis", {}).get("recommended_actions", [])
        
        response = "◊î◊ë◊†◊™◊ô ◊ê◊™ ◊î◊û◊¶◊ë. "
        
        if urgent:
            response += f"◊ô◊© {len(urgent)} ◊ì◊ë◊®◊ô◊ù ◊ì◊ó◊ï◊§◊ô◊ù ◊©◊¶◊®◊ô◊ö ◊ú◊ò◊§◊ú ◊ë◊î◊ù. "
        
        if recommendations:
            response += "◊î◊†◊î ◊û◊î ◊©◊ê◊†◊ô ◊û◊û◊ú◊ô◊•: "
            for i, rec in enumerate(recommendations[:3], 1):
                response += f"\\n{i}. {rec['action']} ({rec['reason']})"
        
        return response

# Create global instance
orchestrator = LifeOrchestrator()

# FastAPI integration
async def process_request(message: str, context: Dict = None) -> Dict:
    """Process incoming request through orchestrator"""
    return await orchestrator.process_message(message, context or {})
`;

// Create directories
if (!fs.existsSync('ai_agent')) {
    fs.mkdirSync('ai_agent');
}

// Write new Python agent
fs.writeFileSync('ai_agent/life_orchestrator.py', pythonAgentContent);
console.log('‚úÖ Created new Life Orchestrator Python agent\n');

// ============= Step 3: Update smart_server.py =============

console.log('üì¶ Step 3: Updating FastAPI server...\n');

const smartServerContent = `"""
FastAPI Server for Life Orchestrator
=====================================
"""

import os
import sys
import json
from datetime import datetime
from typing import Dict, Any, Optional, List
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn
import asyncio

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# Import the Life Orchestrator
try:
    from life_orchestrator import LifeOrchestrator, ContextNode, EntityType
    orchestrator = LifeOrchestrator()
    print("‚úÖ Life Orchestrator loaded successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è Failed to import Life Orchestrator: {e}")
    orchestrator = None

# Create FastAPI app
app = FastAPI(
    title="Life Orchestrator API",
    description="Intelligent Life Management System",
    version="2.0.0"
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Request models
class ChatRequest(BaseModel):
    message: str
    context: Optional[Dict[str, Any]] = {}

class TaskRequest(BaseModel):
    task: Dict[str, Any]

class EmailRequest(BaseModel):
    email: Dict[str, Any]

# ==================== API Endpoints ====================

@app.get("/")
async def root():
    return {
        "status": "active",
        "service": "Life Orchestrator",
        "version": "2.0.0",
        "orchestrator_ready": orchestrator is not None
    }

@app.get("/health")
async def health():
    graph_stats = {}
    memory_stats = {}
    
    if orchestrator:
        graph_stats = {
            "nodes": len(orchestrator.graph.nodes),
            "edges": len(orchestrator.graph.edges),
            "node_types": list(orchestrator.graph.index.keys())
        }
        memory_stats = {
            "short_term": len(orchestrator.memory.short_term),
            "long_term": len(orchestrator.memory.long_term),
            "patterns": len(orchestrator.memory.patterns)
        }
    
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "orchestrator_active": orchestrator is not None,
        "graph": graph_stats,
        "memory": memory_stats
    }

@app.post("/chat")
async def chat(request: ChatRequest):
    """Main chat endpoint"""
    if not orchestrator:
        return {
            "response": "◊û◊¶◊ò◊¢◊®, ◊î◊û◊¢◊®◊õ◊™ ◊ú◊ê ◊ñ◊û◊ô◊†◊î ◊õ◊®◊í◊¢",
            "error": "Orchestrator not initialized"
        }
    
    try:
        result = await orchestrator.process_message(request.message, request.context)
        return result
    except Exception as e:
        print(f"Error in chat: {e}")
        return {
            "response": "◊ê◊ô◊®◊¢◊î ◊©◊í◊ô◊ê◊î ◊ë◊¢◊ô◊ë◊ï◊ì ◊î◊î◊ï◊ì◊¢◊î",
            "error": str(e)
        }

@app.post("/ingest/task")
async def ingest_task(request: TaskRequest):
    """Ingest a new task"""
    if not orchestrator:
        return {"error": "Orchestrator not initialized"}
    
    try:
        perception = await orchestrator.perceive({"task": request.task})
        return {
            "success": True,
            "perception": perception
        }
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.post("/ingest/email")
async def ingest_email(request: EmailRequest):
    """Ingest an email"""
    if not orchestrator:
        return {"error": "Orchestrator not initialized"}
    
    try:
        perception = await orchestrator.perceive({"email": request.email})
        return {
            "success": True,
            "perception": perception
        }
    except Exception as e:
        return {"success": False, "error": str(e)}

@app.get("/state")
async def get_state():
    """Get current system state"""
    if not orchestrator:
        return {"error": "Orchestrator not initialized"}
    
    decision = await orchestrator.decide()
    
    return {
        "graph": {
            "nodes": len(orchestrator.graph.nodes),
            "edges": len(orchestrator.graph.edges),
            "index": {k.value: len(v) for k, v in orchestrator.graph.index.items()}
        },
        "memory": {
            "short_term": len(orchestrator.memory.short_term),
            "long_term": len(orchestrator.memory.long_term),
            "episodic": len(orchestrator.memory.episodic),
            "patterns": len(orchestrator.memory.patterns)
        },
        "analysis": decision.get("analysis", {}),
        "timestamp": datetime.now().isoformat()
    }

@app.post("/learn")
async def learn_from_feedback(feedback: Dict[str, Any]):
    """Submit feedback for learning"""
    if not orchestrator:
        return {"error": "Orchestrator not initialized"}
    
    try:
        await orchestrator.learn(feedback)
        return {"success": True, "message": "Feedback processed"}
    except Exception as e:
        return {"success": False, "error": str(e)}

if __name__ == "__main__":
    host = os.getenv("SMART_SERVER_HOST", "0.0.0.0")
    port = int(os.getenv("SMART_SERVER_PORT", "8000"))
    
    print("üöÄ Starting Life Orchestrator Server...")
    print(f"üì° API available at: http://{host}:{port}")
    print(f"üìö Documentation: http://{host}:{port}/docs")
    
    uvicorn.run(app, host=host, port=port, log_level="info")
`;

fs.writeFileSync('ai_agent/smart_server.py', smartServerContent);
console.log('‚úÖ Updated FastAPI server for Life Orchestrator\n');

// ============= Step 4: Create requirements.txt =============

console.log('üì¶ Step 4: Creating requirements.txt...\n');

const requirementsContent = `# Life Orchestrator Requirements
fastapi>=0.115.4
uvicorn>=0.32.0
pydantic>=2.9.2
python-dotenv>=1.0.0

# Optional for advanced features
# langgraph>=0.2.48
# langchain>=0.3.7
# langchain-openai>=0.2.5
# neo4j>=5.0.0
# redis>=5.0.0
`;

fs.writeFileSync('ai_agent/requirements.txt', requirementsContent);
console.log('‚úÖ Created requirements.txt\n');

// ============= Step 5: Update package.json scripts =============

console.log('üì¶ Step 5: Updating package.json scripts...\n');

const packagePath = 'package.json';
if (fs.existsSync(packagePath)) {
    const packageJson = JSON.parse(fs.readFileSync(packagePath, 'utf8'));
    
    packageJson.scripts = {
        ...packageJson.scripts,
        "start": "node simple-server.js",
        "start:clean": "node server-clean.js",
        "start:agent": "cd ai_agent && python smart_server.py",
        "start:all": "concurrently \"npm run start\" \"npm run start:agent\"",
        "test:agent": "cd ai_agent && python -m pytest",
        "fix": "node fix_life_orchestrator.js",
        "health": "curl http://localhost:3000/api/health && curl http://localhost:8000/health"
    };
    
    fs.writeFileSync(packagePath, JSON.stringify(packageJson, null, 2));
    console.log('‚úÖ Updated package.json scripts\n');
}

// ============= Step 6: Create startup script =============

console.log('üì¶ Step 6: Creating startup script...\n');

const startupScript = `@echo off
echo ========================================
echo   Life Orchestrator - Complete System
echo ========================================
echo.

echo Starting services...
echo.

REM Start Python agent
echo [1/2] Starting Life Orchestrator Agent...
start "Life Orchestrator Agent" cmd /k "cd ai_agent && python smart_server.py"

timeout /t 3 /nobreak > nul

REM Start Node.js server
echo [2/2] Starting Node.js Server...
start "Node.js Server" cmd /k "node simple-server.js"

timeout /t 3 /nobreak > nul

echo.
echo ‚úÖ All services started!
echo.
echo üåê Dashboard: http://localhost:3000
echo ü§ñ AI Agent: http://localhost:8000
echo üìö API Docs: http://localhost:8000/docs
echo.
echo Press any key to open the dashboard...
pause > nul
start http://localhost:3000
`;

fs.writeFileSync('start_orchestrator.bat', startupScript);
console.log('‚úÖ Created startup script\n');

// ============= Step 7: Verification =============

console.log('üîç Step 7: Verifying fixes...\n');

const verificationChecks = [
    { file: 'services/AgentCore.js', desc: 'AgentCore with full functionality' },
    { file: 'ai_agent/life_orchestrator.py', desc: 'Life Orchestrator Python agent' },
    { file: 'ai_agent/smart_server.py', desc: 'Updated FastAPI server' },
    { file: 'ai_agent/requirements.txt', desc: 'Python requirements' },
    { file: 'start_orchestrator.bat', desc: 'Startup script' }
];

let allGood = true;
verificationChecks.forEach(check => {
    if (fs.existsSync(check.file)) {
        console.log(`‚úÖ ${check.desc}: OK`);
    } else {
        console.log(`‚ùå ${check.desc}: MISSING`);
        allGood = false;
    }
});

console.log(`
========================================
           Fix Complete!
========================================

${allGood ? '‚úÖ All fixes applied successfully!' : '‚ö†Ô∏è Some issues remain - check logs above'}

To start the system:
1. Install Python requirements:
   cd ai_agent && pip install -r requirements.txt

2. Run the startup script:
   start_orchestrator.bat

Or manually:
   - Terminal 1: cd ai_agent && python smart_server.py
   - Terminal 2: node simple-server.js

The system now includes:
‚úÖ Complete AgentCore with Memory & Knowledge Graph
‚úÖ Life Orchestrator with Context Graph
‚úÖ Decision Engine with prioritization
‚úÖ Pattern recognition and learning
‚úÖ Full API integration

Dashboard: http://localhost:3000
API Docs: http://localhost:8000/docs
`);